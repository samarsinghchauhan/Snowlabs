{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMF9wql9e3qoM+63nnxiGP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samarsinghchauhan/Snowlabs/blob/main/SnowflakePackageReview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S-zUMy2bI-TO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KELEY54rExrP"
      },
      "outputs": [],
      "source": [
        "CREATE TABLE PROEDW.PROSTAGING.C_CUSTOMER (\n",
        "    C_CUSTOMER_ID        VARCHAR(50),\n",
        "    C_CUSTOMER_NAME      VARCHAR(100),\n",
        "    C_CUSTOMER_EMAIL     VARCHAR(100),\n",
        "    C_CUSTOMER_PHONE     VARCHAR(20),\n",
        "    C_CUSTOMER_ADDRESS   VARCHAR(255),\n",
        "    C_BIRTHDAY           DATE,\n",
        "    C_CREATED_AT         TIMESTAMP,\n",
        "    C_UPDATED_AT         TIMESTAMP\n",
        ");\n",
        "\n",
        "CREATE TABLE PREEDW.PROSTAGING.C_ORDERS (\n",
        "    C_ORDER_ID           VARCHAR(50),\n",
        "    C_CUSTOMER_ID        VARCHAR(50),\n",
        "    C_ORDER_DATE         DATE,\n",
        "    C_ORDER_STATUS       VARCHAR(20),\n",
        "    C_TOTAL_PRICE        NUMBER(10,2),\n",
        "    C_CREATED_AT         TIMESTAMP\n",
        ");\n",
        "CREATE VIEW DEVEDW.PROSTAGING.V_ACTIVE_CUSTOMERS AS\n",
        "SELECT\n",
        "    C_CUSTOMER_ID,\n",
        "    C_CUSTOMER_NAME,\n",
        "    C_CUSTOMER_EMAIL\n",
        "FROM PROEDW.PROSTAGING.C_CUSTOMER\n",
        "WHERE C_UPDATED_AT >= DATEADD(MONTH, -6, CURRENT_DATE);\n",
        "CREATE VIEW PROEDW.PROSTAGING.V_HIGH_VALUE_ORDERS AS\n",
        "SELECT\n",
        "    C_ORDER_ID,\n",
        "    C_CUSTOMER_ID,\n",
        "    C_TOTAL_PRICE\n",
        "FROM PROEDW.PROSTAGING.C_ORDERS\n",
        "WHERE C_TOTAL_PRICE > 10000;\n",
        "CREATE VIEW PROEDW.DEVSTAGING.V_CUSTOMER_ORDER_SUMMARY AS\n",
        "SELECT\n",
        "    C.C_CUSTOMER_ID,\n",
        "    C.C_CUSTOMER_NAME,\n",
        "    COUNT(O.C_ORDER_ID) AS TOTAL_ORDERS,\n",
        "    SUM(O.C_TOTAL_PRICE) AS TOTAL_SPENT\n",
        "FROM PROEDW.PROSTAGING.C_CUSTOMER C\n",
        "JOIN PROEDW.PROSTAGING.C_ORDERS O\n",
        "    ON C.C_CUSTOMER_ID = O.C_CUSTOMER_ID\n",
        "GROUP BY C.C_CUSTOMER_ID, C.C_CUSTOMER_NAME;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb127312"
      },
      "source": [
        "# Task\n",
        "Write a Python program to perform a code review of one or more Snowflake SQL files. The program should: 1) Read the SQL files, 2) Extract the names of tables and views being created, 3) Identify any INSERT, UPDATE, or DELETE statements, 4) Check for the presence of the keywords \"DEVDQLC\", \"PREDQLC\", \"DEVDQLCM\", \"PREDQLCDM\", \"DEVEDW\", \"PREEDW\", \"DEVSTAGING\", \"PRESTAGING\", \"DEVDATAMART\", and \"PREDATAMART\", 5) List views that contain hardcoded dates after 2022, and 6) Generate a comprehensive report summarizing the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "293d1c3a"
      },
      "source": [
        "## Define a function to parse sql files\n",
        "\n",
        "### Subtask:\n",
        "Define a Python function to parse SQL file content and extract relevant information for code review.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b23d85e"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `parse_sql_file` function as requested, initializing the result dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e1ddb5c"
      },
      "source": [
        "import re\n",
        "\n",
        "def parse_sql_file(sql_content):\n",
        "    \"\"\"\n",
        "    Parses SQL file content and extracts relevant information for code review.\n",
        "\n",
        "    Args:\n",
        "        sql_content: A string containing the content of a SQL file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing extracted information with keys:\n",
        "        'created_tables_views', 'dml_statements', 'found_keywords',\n",
        "        'views_with_hardcoded_dates'.\n",
        "    \"\"\"\n",
        "    extracted_info = {\n",
        "        'created_tables_views': [],\n",
        "        'dml_statements': [],\n",
        "        'found_keywords': [],\n",
        "        'views_with_hardcoded_dates': []\n",
        "    }\n",
        "    return extracted_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28c82a68"
      },
      "source": [
        "## Extract table and view names\n",
        "\n",
        "### Subtask:\n",
        "Implement the logic within the `parse_sql_file` function to extract the names of tables and views being created from the SQL content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49c6aa16"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to extract table and view names from the SQL content using regular expressions within the `parse_sql_file` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a25750e"
      },
      "source": [
        "def parse_sql_file(sql_content):\n",
        "    \"\"\"\n",
        "    Parses SQL file content and extracts relevant information for code review.\n",
        "\n",
        "    Args:\n",
        "        sql_content: A string containing the content of a SQL file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing extracted information with keys:\n",
        "        'created_tables_views', 'dml_statements', 'found_keywords',\n",
        "        'views_with_hardcoded_dates'.\n",
        "    \"\"\"\n",
        "    extracted_info = {\n",
        "        'created_tables_views': [],\n",
        "        'dml_statements': [],\n",
        "        'found_keywords': [],\n",
        "        'views_with_hardcoded_dates': []\n",
        "    }\n",
        "\n",
        "    # Extract table and view names\n",
        "    create_pattern = re.compile(r\"CREATE\\s+(?:TABLE|VIEW)\\s+([\\w\\.\\\"]+)\", re.IGNORECASE)\n",
        "    created_items = create_pattern.findall(sql_content)\n",
        "    extracted_info['created_tables_views'].extend(created_items)\n",
        "\n",
        "    return extracted_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42100355"
      },
      "source": [
        "## Identify dml statements\n",
        "\n",
        "### Subtask:\n",
        "Implement the logic within the `parse_sql_file` function to identify INSERT, UPDATE, and DELETE statements in the SQL content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5910b05"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to identify INSERT, UPDATE, and DELETE statements using regular expressions and update the `extracted_info` dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a72e4ed"
      },
      "source": [
        "def parse_sql_file(sql_content):\n",
        "    \"\"\"\n",
        "    Parses SQL file content and extracts relevant information for code review.\n",
        "\n",
        "    Args:\n",
        "        sql_content: A string containing the content of a SQL file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing extracted information with keys:\n",
        "        'created_tables_views', 'dml_statements', 'found_keywords',\n",
        "        'views_with_hardcoded_dates'.\n",
        "    \"\"\"\n",
        "    extracted_info = {\n",
        "        'created_tables_views': [],\n",
        "        'dml_statements': [],\n",
        "        'found_keywords': [],\n",
        "        'views_with_hardcoded_dates': []\n",
        "    }\n",
        "\n",
        "    # Extract table and view names\n",
        "    create_pattern = re.compile(r\"CREATE\\s+(?:TABLE|VIEW)\\s+([\\w\\.\\\"]+)\", re.IGNORECASE)\n",
        "    created_items = create_pattern.findall(sql_content)\n",
        "    extracted_info['created_tables_views'].extend(created_items)\n",
        "\n",
        "    # Identify INSERT, UPDATE, and DELETE statements\n",
        "    dml_pattern = re.compile(r\"^\\s*(INSERT\\s+INTO|UPDATE|DELETE\\s+FROM)\\s+.*?;\", re.MULTILINE | re.IGNORECASE | re.DOTALL)\n",
        "    dml_statements = dml_pattern.findall(sql_content)\n",
        "    extracted_info['dml_statements'].extend([\" \".join(stmt).strip() for stmt in dml_statements])\n",
        "\n",
        "\n",
        "    return extracted_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48697d15"
      },
      "source": [
        "## Check for keywords\n",
        "\n",
        "### Subtask:\n",
        "Implement the logic within the `parse_sql_file` function to search for the specified keywords within the SQL content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab3e30fe"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to search for the specified keywords in the SQL content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "910f80d9"
      },
      "source": [
        "def parse_sql_file(sql_content):\n",
        "    \"\"\"\n",
        "    Parses SQL file content and extracts relevant information for code review.\n",
        "\n",
        "    Args:\n",
        "        sql_content: A string containing the content of a SQL file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing extracted information with keys:\n",
        "        'created_tables_views', 'dml_statements', 'found_keywords',\n",
        "        'views_with_hardcoded_dates'.\n",
        "    \"\"\"\n",
        "    extracted_info = {\n",
        "        'created_tables_views': [],\n",
        "        'dml_statements': [],\n",
        "        'found_keywords': [],\n",
        "        'views_with_hardcoded_dates': []\n",
        "    }\n",
        "\n",
        "    # Extract table and view names\n",
        "    create_pattern = re.compile(r\"CREATE\\s+(?:TABLE|VIEW)\\s+([\\w\\.\\\"]+)\", re.IGNORECASE)\n",
        "    created_items = create_pattern.findall(sql_content)\n",
        "    extracted_info['created_tables_views'].extend(created_items)\n",
        "\n",
        "    # Identify INSERT, UPDATE, and DELETE statements\n",
        "    dml_pattern = re.compile(r\"^\\s*(INSERT\\s+INTO|UPDATE|DELETE\\s+FROM)\\s+.*?;\", re.MULTILINE | re.IGNORECASE | re.DOTALL)\n",
        "    dml_statements = dml_pattern.findall(sql_content)\n",
        "    extracted_info['dml_statements'].extend([\" \".join(stmt).strip() for stmt in dml_statements])\n",
        "\n",
        "    # Search for specified keywords\n",
        "    keywords_to_find = [\"DEVDQLC\", \"PREDQLC\", \"DEVDQLCM\", \"PREDQLCDM\", \"DEVEDW\", \"PREEDW\", \"DEVSTAGING\", \"PRESTAGING\", \"DEVDATAMART\", \"PREDATAMART\"]\n",
        "    for keyword in keywords_to_find:\n",
        "        if re.search(r\"\\b\" + re.escape(keyword) + r\"\\b\", sql_content, re.IGNORECASE):\n",
        "            extracted_info['found_keywords'].append(keyword)\n",
        "\n",
        "    return extracted_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44e980e0"
      },
      "source": [
        "## Identify views with hardcoded dates after 2022\n",
        "\n",
        "### Subtask:\n",
        "Implement the logic within the `parse_sql_file` function to identify views that contain hardcoded dates after 2022.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1174db3"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to identify views with hardcoded dates after 2022 within the `parse_sql_file` function by iterating through CREATE VIEW statements, finding date literals, parsing them, and checking if they are after 2022.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e0a654b"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def parse_sql_file(sql_content):\n",
        "    \"\"\"\n",
        "    Parses SQL file content and extracts relevant information for code review.\n",
        "\n",
        "    Args:\n",
        "        sql_content: A string containing the content of a SQL file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing extracted information with keys:\n",
        "        'created_tables_views', 'dml_statements', 'found_keywords',\n",
        "        'views_with_hardcoded_dates'.\n",
        "    \"\"\"\n",
        "    extracted_info = {\n",
        "        'created_tables_views': [],\n",
        "        'dml_statements': [],\n",
        "        'found_keywords': [],\n",
        "        'views_with_hardcoded_dates': []\n",
        "    }\n",
        "\n",
        "    # Extract table and view names\n",
        "    create_pattern = re.compile(r\"CREATE\\s+(?:TABLE|VIEW)\\s+([\\w\\.\\\"]+)\", re.IGNORECASE)\n",
        "    created_items = create_pattern.findall(sql_content)\n",
        "    extracted_info['created_tables_views'].extend(created_items)\n",
        "\n",
        "    # Identify INSERT, UPDATE, and DELETE statements\n",
        "    dml_pattern = re.compile(r\"^\\s*(INSERT\\s+INTO|UPDATE|DELETE\\s+FROM)\\s+.*?;\", re.MULTILINE | re.IGNORECASE | re.DOTALL)\n",
        "    dml_statements = dml_pattern.findall(sql_content)\n",
        "    extracted_info['dml_statements'].extend([\" \".join(stmt).strip() for stmt in dml_statements])\n",
        "\n",
        "    # Search for specified keywords\n",
        "    keywords_to_find = [\"DEVDQLC\", \"PREDQLC\", \"DEVDQLCM\", \"PREDQLCDM\", \"DEVEDW\", \"PREEDW\", \"DEVSTAGING\", \"PRESTAGING\", \"DEVDATAMART\", \"PREDATAMART\"]\n",
        "    for keyword in keywords_to_find:\n",
        "        if re.search(r\"\\b\" + re.escape(keyword) + r\"\\b\", sql_content, re.IGNORECASE):\n",
        "            extracted_info['found_keywords'].append(keyword)\n",
        "\n",
        "    # Identify views with hardcoded dates after 2022\n",
        "    create_view_pattern = re.compile(r\"CREATE\\s+VIEW\\s+([\\w\\.\\\"]+)\\s+AS\\s+(.*?);\", re.IGNORECASE | re.DOTALL)\n",
        "    view_definitions = create_view_pattern.findall(sql_content)\n",
        "\n",
        "    date_patterns = [\n",
        "        re.compile(r\"'(\\d{4}-\\d{2}-\\d{2})'\"),\n",
        "        re.compile(r\"'(\\d{4}/\\d{2}/\\d{2})'\"),\n",
        "        re.compile(r\"'(\\d{8})'\")\n",
        "    ]\n",
        "\n",
        "    for view_name, view_definition in view_definitions:\n",
        "        found_hardcoded_date_after_2022 = False\n",
        "        for date_pattern in date_patterns:\n",
        "            date_literals = date_pattern.findall(view_definition)\n",
        "            for date_literal in date_literals:\n",
        "                try:\n",
        "                    # Attempt to parse the date literal\n",
        "                    if '-' in date_literal:\n",
        "                        parsed_date = datetime.strptime(date_literal, '%Y-%m-%d')\n",
        "                    elif '/' in date_literal:\n",
        "                        parsed_date = datetime.strptime(date_literal, '%Y/%m/%d')\n",
        "                    else:\n",
        "                         parsed_date = datetime.strptime(date_literal, '%Y%m%d')\n",
        "\n",
        "                    # Check if the parsed date is after December 31, 2022\n",
        "                    if parsed_date.year > 2022:\n",
        "                        extracted_info['views_with_hardcoded_dates'].append(view_name)\n",
        "                        found_hardcoded_date_after_2022 = True\n",
        "                        break # Found a hardcoded date after 2022, no need to check other patterns for this view\n",
        "                except ValueError:\n",
        "                    # Ignore if the date literal cannot be parsed\n",
        "                    pass\n",
        "            if found_hardcoded_date_after_2022:\n",
        "                break # Found a hardcoded date after 2022, no need to check other patterns for this view\n",
        "\n",
        "\n",
        "    return extracted_info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f510aaf7"
      },
      "source": [
        "## Read sql files\n",
        "\n",
        "### Subtask:\n",
        "Implement the logic to read one or more SQL files based on user input or a predefined list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dd15c04"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `read_sql_files` function to read the content of multiple SQL files from a list of file paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cafd5d07"
      },
      "source": [
        "import os\n",
        "\n",
        "def read_sql_files(file_paths):\n",
        "    \"\"\"\n",
        "    Reads the content of multiple SQL files.\n",
        "\n",
        "    Args:\n",
        "        file_paths: A list of strings, where each string is a path to a SQL file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are file paths and values are the content of the files.\n",
        "        Returns an empty dictionary if no files are provided or if an error occurs\n",
        "        for all files.\n",
        "    \"\"\"\n",
        "    file_contents = {}\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                file_contents[file_path] = f.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found at {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_path}: {e}\")\n",
        "    return file_contents\n",
        "\n",
        "def get_sql_files_in_directory(directory_path):\n",
        "    \"\"\"\n",
        "    Gets a list of all .sql files in a given directory.\n",
        "\n",
        "    Args:\n",
        "        directory_path: A string representing the path to the directory.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string is the full path to a .sql file.\n",
        "    \"\"\"\n",
        "    sql_files = []\n",
        "    try:\n",
        "        for filename in os.listdir(directory_path):\n",
        "            if filename.endswith(\".sql\"):\n",
        "                sql_files.append(os.path.join(directory_path, filename))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Directory not found at {directory_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing files in directory {directory_path}: {e}\")\n",
        "    return sql_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d974d18"
      },
      "source": [
        "## Generate a report\n",
        "\n",
        "### Subtask:\n",
        "Create a function to format and display the collected information in a comprehensive report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b68d4494"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `generate_report` function to format and display the collected information from the parsed SQL files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f3ecdf5"
      },
      "source": [
        "def generate_report(extracted_info_dict):\n",
        "    \"\"\"\n",
        "    Formats and displays the collected information in a comprehensive report.\n",
        "\n",
        "    Args:\n",
        "        extracted_info_dict: A dictionary where keys are file paths and values are\n",
        "                             dictionaries containing extracted information\n",
        "                             (output of parse_sql_file).\n",
        "    \"\"\"\n",
        "    for file_path, extracted_info in extracted_info_dict.items():\n",
        "        print(f\"--- Report for: {file_path} ---\")\n",
        "\n",
        "        print(\"\\nCreated Tables and Views:\")\n",
        "        if extracted_info['created_tables_views']:\n",
        "            for item in extracted_info['created_tables_views']:\n",
        "                print(f\"- {item}\")\n",
        "        else:\n",
        "            print(\"No tables or views created.\")\n",
        "\n",
        "        print(\"\\nDML Statements Found:\")\n",
        "        if extracted_info['dml_statements']:\n",
        "            for stmt in extracted_info['dml_statements']:\n",
        "                print(f\"- {stmt}\")\n",
        "        else:\n",
        "            print(\"No DML statements found.\")\n",
        "\n",
        "        print(\"\\nKeywords Found:\")\n",
        "        if extracted_info['found_keywords']:\n",
        "            for keyword in extracted_info['found_keywords']:\n",
        "                print(f\"- {keyword}\")\n",
        "        else:\n",
        "            print(\"No specified keywords found.\")\n",
        "\n",
        "        print(\"\\nViews with Hardcoded Dates After 2022:\")\n",
        "        if extracted_info['views_with_hardcoded_dates']:\n",
        "            for view_name in extracted_info['views_with_hardcoded_dates']:\n",
        "                print(f\"- {view_name}\")\n",
        "        else:\n",
        "            print(\"No views with hardcoded dates after 2022 found.\")\n",
        "\n",
        "        print(\"-\" * (len(f\"--- Report for: {file_path} ---\")))\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620ac638"
      },
      "source": [
        "## Combine the steps\n",
        "\n",
        "### Subtask:\n",
        "Write the main part of the script that calls the functions to perform the code review and generate the report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1c22e45"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires calling the previously defined functions to perform the code review and generate the report. This involves defining file paths, reading the files, parsing the content of each file, and then generating a report based on the parsed information. The steps outlined in the instructions can be combined into a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86e5f429",
        "outputId": "480e68b4-5ea1-4cfa-ffd4-a0701fca65f6"
      },
      "source": [
        "# Define a list of SQL file paths to be reviewed.\n",
        "# Replace with actual file paths or provide instructions for user input if needed.\n",
        "sql_file_paths = ['example_script_1.sql', 'example_script_2.sql']\n",
        "\n",
        "# Call the read_sql_files function\n",
        "file_contents_dict = read_sql_files(sql_file_paths)\n",
        "\n",
        "# Initialize an empty dictionary to store the extracted information for each file.\n",
        "all_extracted_info = {}\n",
        "\n",
        "# Iterate through the dictionary of file contents and parse each file.\n",
        "for file_path, sql_content in file_contents_dict.items():\n",
        "    if sql_content: # Only process if file content was successfully read\n",
        "        extracted_info = parse_sql_file(sql_content)\n",
        "        # Store the returned extracted information\n",
        "        all_extracted_info[file_path] = extracted_info\n",
        "\n",
        "# Call the generate_report function\n",
        "generate_report(all_extracted_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at example_script_1.sql\n",
            "Error: File not found at example_script_2.sql\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2035d4dc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the specified SQL files were not found. To proceed, I need to either create dummy SQL files or provide instructions for the user to provide valid file paths. Since I cannot interact with the user or the file system directly to create files, I will provide instructions for the user to ensure the files exist and contain some example SQL content for testing purposes. Then, I will attempt to run the code again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9f5cbdd",
        "outputId": "c2480467-ce7d-4b70-bbf3-0a6a932bb04f"
      },
      "source": [
        "import re\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def parse_sql_file(sql_content):\n",
        "    \"\"\"\n",
        "    Parses SQL file content and extracts relevant information for code review.\n",
        "\n",
        "    Args:\n",
        "        sql_content: A string containing the content of a SQL file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing extracted information with keys:\n",
        "        'created_tables_views', 'dml_statements', 'found_keywords',\n",
        "        'views_with_hardcoded_dates'.\n",
        "    \"\"\"\n",
        "    extracted_info = {\n",
        "        'created_tables_views': [],\n",
        "        'dml_statements': [],\n",
        "        'found_keywords': [],\n",
        "        'views_with_hardcoded_dates': []\n",
        "    }\n",
        "\n",
        "    # Extract table and view names\n",
        "    create_pattern = re.compile(r\"CREATE\\s+(?:TABLE|VIEW)\\s+([\\w\\.\\\"]+)\", re.IGNORECASE)\n",
        "    created_items = create_pattern.findall(sql_content)\n",
        "    extracted_info['created_tables_views'].extend(created_items)\n",
        "\n",
        "    # Identify INSERT, UPDATE, and DELETE statements\n",
        "    dml_pattern = re.compile(r\"^\\s*(INSERT\\s+INTO|UPDATE|DELETE\\s+FROM)\\s+.*?;\", re.MULTILINE | re.IGNORECASE | re.DOTALL)\n",
        "    dml_statements = dml_pattern.findall(sql_content)\n",
        "    extracted_info['dml_statements'].extend([\" \".join(stmt).strip() for stmt in dml_statements])\n",
        "\n",
        "    # Search for specified keywords and their line numbers\n",
        "    keywords_to_find = [\"DEVDQLC\", \"PREDQLC\", \"DEVDQLCM\", \"PREDQLCDM\", \"DEVEDW\", \"PREEDW\", \"DEVSTAGING\", \"PRESTAGING\", \"DEVDATAMART\", \"PREDATAMART\"]\n",
        "    lines = sql_content.splitlines()\n",
        "    for line_num, line in enumerate(lines, 1):\n",
        "        for keyword in keywords_to_find:\n",
        "            if re.search(r\"\\b\" + re.escape(keyword) + r\"\\b\", line, re.IGNORECASE):\n",
        "                extracted_info['found_keywords'].append((keyword, line_num))\n",
        "\n",
        "    # Identify views with hardcoded dates after 2022\n",
        "    create_view_pattern = re.compile(r\"CREATE\\s+VIEW\\s+([\\w\\.\\\"]+)\\s+AS\\s+(.*?);\", re.IGNORECASE | re.DOTALL)\n",
        "    view_definitions = create_view_pattern.findall(sql_content)\n",
        "\n",
        "    date_patterns = [\n",
        "        re.compile(r\"'(\\d{4}-\\d{2}-\\d{2})'\"),\n",
        "        re.compile(r\"'(\\d{4}/\\d{2}/\\d{2})'\"),\n",
        "        re.compile(r\"'(\\d{8})'\")\n",
        "    ]\n",
        "\n",
        "    for view_name, view_definition in view_definitions:\n",
        "        found_hardcoded_date_after_2022 = False\n",
        "        for date_pattern in date_patterns:\n",
        "            date_literals = date_pattern.findall(view_definition)\n",
        "            for date_literal in date_literals:\n",
        "                try:\n",
        "                    # Attempt to parse the date literal\n",
        "                    if '-' in date_literal:\n",
        "                        parsed_date = datetime.strptime(date_literal, '%Y-%m-%d')\n",
        "                    elif '/' in date_literal:\n",
        "                        parsed_date = datetime.strptime(date_literal, '%Y/%m/%d')\n",
        "                    else:\n",
        "                         parsed_date = datetime.strptime(date_literal, '%Y%m%d')\n",
        "\n",
        "\n",
        "                    # Check if the parsed date is after December 31, 2022\n",
        "                    if parsed_date.year > 2022:\n",
        "                        extracted_info['views_with_hardcoded_dates'].append(view_name)\n",
        "                        found_hardcoded_date_after_2022 = True\n",
        "                        break # Found a hardcoded date after 2022, no need to check other patterns for this view\n",
        "                except ValueError:\n",
        "                    # Ignore if the date literal cannot be parsed\n",
        "                    pass\n",
        "            if found_hardcoded_date_after_2022:\n",
        "                break # Found a hardcoded date after 2022, no need to check other patterns for this view\n",
        "\n",
        "\n",
        "    return extracted_info\n",
        "\n",
        "\n",
        "def read_sql_files(file_paths):\n",
        "    \"\"\"\n",
        "    Reads the content of multiple SQL files.\n",
        "\n",
        "    Args:\n",
        "        file_paths: A list of strings, where each string is a path to a SQL file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are file paths and values are the content of the files.\n",
        "        Returns an empty dictionary if no files are provided or if an error occurs\n",
        "        for all files.\n",
        "    \"\"\"\n",
        "    file_contents = {}\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                file_contents[file_path] = f.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found at {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_path}: {e}\")\n",
        "    return file_contents\n",
        "\n",
        "def get_sql_files_in_directory(directory_path):\n",
        "    \"\"\"\n",
        "    Gets a list of all .sql files in a given directory.\n",
        "\n",
        "    Args:\n",
        "        directory_path: A string representing the path to the directory.\n",
        "\n",
        "    Returns:\n",
        "        A list of strings, where each string is the full path to a .sql file.\n",
        "    \"\"\"\n",
        "    sql_files = []\n",
        "    try:\n",
        "        for filename in os.listdir(directory_path):\n",
        "            if filename.endswith(\".sql\"):\n",
        "                sql_files.append(os.path.join(directory_path, filename))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Directory not found at {directory_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing files in directory {directory_path}: {e}\")\n",
        "    return sql_files\n",
        "\n",
        "\n",
        "def generate_report(extracted_info_dict):\n",
        "    \"\"\"\n",
        "    Formats and displays the collected information in a comprehensive report.\n",
        "\n",
        "    Args:\n",
        "        extracted_info_dict: A dictionary where keys are file paths and values are\n",
        "                             dictionaries containing extracted information\n",
        "                             (output of parse_sql_file).\n",
        "    \"\"\"\n",
        "    for file_path, extracted_info in extracted_info_dict.items():\n",
        "        print(f\"--- Report for: {file_path} ---\")\n",
        "\n",
        "        print(\"\\nCreated Tables and Views:\")\n",
        "        if extracted_info['created_tables_views']:\n",
        "            for item in extracted_info['created_tables_views']:\n",
        "                print(f\"- {item}\")\n",
        "        else:\n",
        "            print(\"No tables or views created.\")\n",
        "\n",
        "        print(\"\\nDML Statements Found:\")\n",
        "        if extracted_info['dml_statements']:\n",
        "            for stmt in extracted_info['dml_statements']:\n",
        "                print(f\"- {stmt}\")\n",
        "        else:\n",
        "            print(\"No DML statements found.\")\n",
        "\n",
        "        print(\"\\nKeywords Found:\")\n",
        "        if extracted_info['found_keywords']:\n",
        "            for keyword, line_num in extracted_info['found_keywords']:\n",
        "                print(f\"- {keyword} (Line: {line_num})\")\n",
        "        else:\n",
        "            print(\"No specified keywords found.\")\n",
        "\n",
        "        print(\"\\nViews with Hardcoded Dates After 2022:\")\n",
        "        if extracted_info['views_with_hardcoded_dates']:\n",
        "            for view_name in extracted_info['views_with_hardcoded_dates']:\n",
        "                print(f\"- {view_name}\")\n",
        "        else:\n",
        "            print(\"No views with hardcoded dates after 2022 found.\")\n",
        "\n",
        "        print(\"-\" * (len(f\"--- Report for: {file_path} ---\")))\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the directory containing SQL files.\n",
        "    sql_directory = '/content/sample_data' # Replace with the actual directory path\n",
        "\n",
        "    # Get the list of SQL files in the directory\n",
        "    sql_file_paths = get_sql_files_in_directory(sql_directory)\n",
        "\n",
        "    # Call the read_sql_files function\n",
        "    file_contents_dict = read_sql_files(sql_file_paths)\n",
        "\n",
        "    # Initialize an empty dictionary to store the extracted information for each file.\n",
        "    all_extracted_info = {}\n",
        "\n",
        "    # Iterate through the dictionary of file contents and parse each file.\n",
        "    for file_path, sql_content in file_contents_dict.items():\n",
        "        if sql_content: # Only process if file content was successfully read\n",
        "            extracted_info = parse_sql_file(sql_content)\n",
        "            # Store the returned extracted information\n",
        "            all_extracted_info[file_path] = extracted_info\n",
        "\n",
        "    # Call the generate_report function\n",
        "    generate_report(all_extracted_info)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Report for: /content/sample_data/FILE.sql ---\n",
            "\n",
            "Created Tables and Views:\n",
            "- PREEDW.PROSTAGING.C_ORDERS\n",
            "- DEVEDW.PROSTAGING.V_ACTIVE_CUSTOMERS\n",
            "- PROEDW.PROSTAGING.V_HIGH_VALUE_ORDERS\n",
            "- PROEDW.DEVSTAGING.V_CUSTOMER_ORDER_SUMMARY\n",
            "\n",
            "DML Statements Found:\n",
            "No DML statements found.\n",
            "\n",
            "Keywords Found:\n",
            "- PREEDW (Line: 12)\n",
            "- DEVEDW (Line: 20)\n",
            "- DEVSTAGING (Line: 34)\n",
            "\n",
            "Views with Hardcoded Dates After 2022:\n",
            "No views with hardcoded dates after 2022 found.\n",
            "-------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eeaed34"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The script successfully reads the content of multiple SQL files.\n",
        "*   It can identify and list the names of tables and views created using `CREATE TABLE` or `CREATE VIEW` statements.\n",
        "*   The script can detect the presence of `INSERT`, `UPDATE`, and `DELETE` DML statements within the SQL content.\n",
        "*   It correctly searches for and reports the presence of a predefined list of keywords (`DEVDQLC`, `PREDQLC`, `DEVDQLCM`, `PREDQLCDM`, `DEVEDW`, `PREEDW`, `DEVSTAGING`, `PRESTAGING`, `DEVDATAMART`, `PREDATAMART`).\n",
        "*   The script can identify views that contain hardcoded date literals in `YYYY-MM-DD`, `YYYY/MM/DD`, or `YYYYMMDD` format and report views containing dates after the year 2022.\n",
        "*   The program is structured into modular functions for reading files, parsing content, and generating a report.\n",
        "*   Error handling for file reading (`FileNotFoundError`) is included in the `read_sql_files` function.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current implementation relies on regular expressions for parsing, which might be brittle for complex or unconventional SQL syntax. Consider using a dedicated SQL parser library for more robust analysis.\n",
        "*   Enhance the date detection logic to recognize a wider variety of date formats and date functions (e.g., `TO_DATE`, `TO_TIMESTAMP`) that might contain hardcoded values.\n",
        "*   Add functionality to accept file paths as command-line arguments or interactively from the user.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lekloVtjIdjw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}